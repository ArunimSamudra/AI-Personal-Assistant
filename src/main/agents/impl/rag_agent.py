import os
from uuid import uuid4

import chromadb
import ollama
from chromadb import Settings, DEFAULT_TENANT, DEFAULT_DATABASE
from langchain_chroma import Chroma
from langchain_core import documents
from langchain_core.prompts import PromptTemplate
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyMuPDFLoader

from langgraph.prebuilt import create_react_agent
from langchain_core.tools import tool

from main.config import Config


class RAGAgent:
    vectorstore = None
    rag_chain = None
    processed_files = set()

    def __init__(self, model):
        self.rag_agent_prompt = """
                You are part of a multi-agent system designed to be a personal assistant for the user. 
                Specifically, you are the **RAG Agent** responsible reading the pdf documents
                and answering the questions user will have on those documents
            """
        self.rag_agent = create_react_agent(model, tools=[self.rag],
                                            state_modifier=self.rag_agent_prompt)

    def __call__(self, *args, **kwargs):
        return self.rag_agent

    @staticmethod
    @tool
    def rag(query):
        """
            Performs Retrieval-Augmented Generation (RAG) to answer a given query
            based on the documents stored in the vectorstore.

            This method first updates the vectorstore with any newly uploaded PDF files
            from the designated upload folder. It then retrieves relevant context from
            the vectorstore and generates a concise answer using a language model.

            Parameters:
                query (str): The question or query for which an answer is sought.
                             This should be a clear and concise question.

            Returns:
                str: A concise answer generated by the language model based on the
                     retrieved context. If the model cannot provide an answer, it will
                     indicate that it doesn't know the answer.

            Raises:
                Exception: Raises an exception if there is an issue with loading
                           documents, processing the vectorstore, or invoking the
                           language model.

            Notes:
                - This method is designed to dynamically incorporate newly uploaded
                  PDF documents into the knowledge base.
        """
        # Update the vectorstore with any new PDF files
        RAGAgent.update_vectorstore()

        results = RAGAgent.vectorstore.similarity_search(
            query,
            k=5,
        )
        for res in results:
            print(f"* {res.page_content} [{res.metadata}]")

        # Extract the documents from the results
        data1 = results[0].page_content
        data2 = results[1].page_content
        data3 = results[2].page_content
        data4 = results[3].page_content
        data5 = results[4].page_content

        context = f"{data1}\n\n{data2}\n\n{data3}\n\n{data4}\n\n{data5}"

        prompt = PromptTemplate(
            input_variables=["query, context"],
            template=(
                """
                You are an assistant for question-answering tasks. Use the following pieces of retrieved context to 
                answer the question. 
                If you don't know the answer, just say that you don't know. Keep your answers short and to the point.
                Question: {query} 
                Context: {context} 
                Answer:
                """
            ),
        )

        model = ChatOllama(model=Config.LOCAL_LLM)
        response = model.invoke(prompt.format(query=query, context=context))
        print(response)
        return response

    @staticmethod
    def update_vectorstore():
        """Load PDFs from the upload folder and update the vectorstore."""
        pdf_files = [f for f in os.listdir(Config.UPLOAD_PATH) if f.endswith('.pdf')]
        new_files = [f for f in pdf_files if f not in RAGAgent.processed_files]

        for pdf_name in new_files:
            # Load the PDF file
            loader = PyMuPDFLoader(os.path.join(Config.UPLOAD_PATH, pdf_name))
            docs = loader.load()

            # Split documents into chunks
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=400,  # Size of each chunk in characters
                                                           chunk_overlap=100,  # Overlap between consecutive chunks
                                                           length_function=len,
                                                           # Function to compute the length of the text
                                                           add_start_index=True,
                                                           # Flag to add start index to each chunk
                                                           )
            splits = text_splitter.split_documents(docs)

            # Add new splits to the existing vectorstore or create a new one if it doesnâ€™t exist
            if RAGAgent.vectorstore is None:

                embeddings = OllamaEmbeddings(
                    model="llama3.2:1b",
                )
                RAGAgent.vectorstore = Chroma(
                    collection_name="example_collection",
                    embedding_function=embeddings,
                    persist_directory="./chroma_langchain_db",
                    # Where to save data locally, remove if not necessary
                    )

            uuids = [str(uuid4()) for _ in range(len(splits))]

            RAGAgent.vectorstore.add_documents(documents=splits, ids=uuids)

            RAGAgent.processed_files.add(pdf_name)
